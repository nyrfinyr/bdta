{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEZDBAtRNkGD"
   },
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHqUR4F1NkGF"
   },
   "source": [
    "### Standardization, or mean removal and variance scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-lzTNXTNkGG"
   },
   "source": [
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn;\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n8kBGdMJNkGG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "X_scaled = preprocessing.scale(X_train)   # default axis=0 standardize each feature\n",
    "\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0dwh-YHNkGO"
   },
   "source": [
    "The standard score of a sample x is calculated as:\n",
    "<tt>z = (x - u) / s</tt>\n",
    "where u is the mean of the training samples or zero if <tt>with_mean=False</tt>, and s is the standard deviation of the training samples or one if <tt>with_std=False</tt>.\n",
    "\n",
    "\n",
    "The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BQCBHOBzNkGP"
   },
   "outputs": [],
   "source": [
    "sss = preprocessing.StandardScaler()\n",
    "scaler = sss.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g81V64OTNkGZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PXkvsXW8Bvrb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(4.9343245538895844e-17), np.float64(1.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_train).mean(), scaler.transform(X_train).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gCw0tHgNkGf"
   },
   "source": [
    "The scaler instance can then be used on new data to transform it the same way it did on the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtzQRNTDeajd"
   },
   "outputs": [],
   "source": [
    "X_test = np.array([[ 2., -3.,  2.],\n",
    "                    [ 2.,  5.,  0.],\n",
    "                    [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRbkV4K3emLC"
   },
   "outputs": [],
   "source": [
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhU4v1Xr8ZSd"
   },
   "outputs": [],
   "source": [
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOtzJFf5NkGc"
   },
   "outputs": [],
   "source": [
    "scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuqSnnTRNkGh"
   },
   "source": [
    "### Scaling features to a range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB5x4XOGNkGj"
   },
   "source": [
    "An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkb3NZgLNkGj"
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XZVQLdaNkGl"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler((0,1)) # default [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-v5jbBINkGo"
   },
   "outputs": [],
   "source": [
    "min_max_scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DP0CR5gaNkGq"
   },
   "outputs": [],
   "source": [
    "min_max_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpMedwIGNkGx"
   },
   "outputs": [],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ1xksTwEmQL"
   },
   "source": [
    "The transformation is given by:\n",
    "\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l-9AZsRNkHD"
   },
   "source": [
    "If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use robust_scale and RobustScaler (x_scaled = x / max(abs(x)))\n",
    "as drop-in replacements instead. They use more robust estimates for the center and range of your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ExvvyfZANkHD"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "X = [[ 1., -2.,  2.],\n",
    "     [ -2.,  1.,  3.],\n",
    "     [ 4.,  1., -2.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tKo-0I_hNkHG"
   },
   "outputs": [],
   "source": [
    "transformer = RobustScaler().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KKUzJzNl-dDL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3. , 1.5, 2.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eZLEqPrzNkHS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. , -2. ,  0. ],\n",
       "       [-1. ,  0. ,  0.4],\n",
       "       [ 1. ,  0. , -1.6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67eW-aqhIWKy"
   },
   "source": [
    "The transformation is computed as:\n",
    "\n",
    "<tt>X_scaled = (X - X.median(axis=0)) / X.IQR(axis=0)</tt>\n",
    "\n",
    "where <tt>IQR = q3 - q1</tt>\n",
    "\n",
    "The IQR is the range between the 1st quartile (q1) and the 3rd quartile (q3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSiUhq3wNkHU"
   },
   "source": [
    "## Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYDtanXtNkHU"
   },
   "source": [
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "‘l1’\n",
    "The l1 norm uses the sum of all the values and thus gives equal penalty to all parameters, enforcing sparsity.\n",
    "x_normalized = x / sum(X)\n",
    "\n",
    "\n",
    "‘l2’\n",
    "The l2 norm uses the square root of the sum of all the squared values. This creates smoothness and rotational invariance. Some models, like PCA, assume rotational invariance, and so l2 will perform better.\n",
    "x_normalized = x / sqrt(sum((i**2) for i in X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVY0yz-uNkHV"
   },
   "outputs": [],
   "source": [
    "X = np.array([[1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgpfOf3fy7FC"
   },
   "outputs": [],
   "source": [
    "# the same as previous X_normalized = preprocessing.Normalizer(norm='l2').fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE-rv9EeNkHY"
   },
   "outputs": [],
   "source": [
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZvskl7f_EV-"
   },
   "outputs": [],
   "source": [
    "np.sqrt(0.40824829**2 + 0.40824829**2 + 0.81649658**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brWoQTSNJ7Kl"
   },
   "outputs": [],
   "source": [
    "np.sqrt(0**2 + 0.70710678**2 + (-0.70710678)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KCZQAmrxEHP"
   },
   "source": [
    "You can refer to [https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEsK6xb9NkHb"
   },
   "source": [
    "##  Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkG8EfHFSIeg"
   },
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l68BkGzK_-Wl"
   },
   "outputs": [],
   "source": [
    "X = [['male', 'Spain', 'tennis'],\n",
    "     ['female', 'Italy', 'football'],\n",
    "     ['female', 'Italy', 'painting'],\n",
    "     ['male', 'Spain', 'tennis'],\n",
    "     ['female', 'France', 'football'],\n",
    "     ['male', 'Italy', 'painting']]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i51eTlyFh7cX"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X, columns=['gender', 'locations', 'hobby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSWKCxGRNkHb"
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGygA88zKoRZ"
   },
   "outputs": [],
   "source": [
    "enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eat_IC5PjDQ"
   },
   "outputs": [],
   "source": [
    "enc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9th2WJwNkHh"
   },
   "outputs": [],
   "source": [
    "enc.transform([['male', 'Italy', 'football'],\n",
    "               ['female', 'France', 'painting']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMJUhjNnNEnN"
   },
   "outputs": [],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzwyZ_k0A0JX"
   },
   "outputs": [],
   "source": [
    "new_features = []\n",
    "for i in enc.categories_:\n",
    "  for j in i:\n",
    "    new_features.append(j)\n",
    "\n",
    "\n",
    "Xnew = enc.transform(X).toarray()\n",
    "pd.DataFrame(Xnew, columns=new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V1Vy9yAipjK"
   },
   "source": [
    "#### with fixed categories value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLibdqXaNkHl"
   },
   "outputs": [],
   "source": [
    "genders = ['female', 'male']\n",
    "locations = ['Spain', 'France', 'Italy']\n",
    "hobbies = ['football', 'painting', 'tennis']\n",
    "enc = preprocessing.OneHotEncoder(categories=[genders, locations, hobbies])\n",
    "\n",
    "X = [['male', 'Spain', 'tennis'],\n",
    "     ['female', 'Italy', 'football'],\n",
    "     ['female', 'Italy', 'painting'],\n",
    "     ['male', 'Spain', 'tennis'],\n",
    "     ['female', 'France', 'football'],\n",
    "     ['male', 'Italy', 'painting']]\n",
    "\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i8jFXsSNkHm"
   },
   "outputs": [],
   "source": [
    "enc.transform([['female', 'USA', 'football']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u__9fKWNkHp"
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "X = [['male', 'Spain', 'tennis'],\n",
    "     ['female', 'Italy', 'football'],\n",
    "     ['female', 'Italy', 'painting'],\n",
    "     ['male', 'Spain', 'tennis'],\n",
    "     ['female', 'France', 'football'],\n",
    "     ['male', 'Italy', 'painting']]\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXQWnHZEDozJ"
   },
   "outputs": [],
   "source": [
    "enc.transform([['female', 'USA', 'football']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVNyWnnkj3_r"
   },
   "outputs": [],
   "source": [
    "enc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kkc_oTAGpoxO"
   },
   "outputs": [],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj10SiSCD0KT"
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=2)\n",
    "\n",
    "X = [['male', 'Spain', 'tennis'],\n",
    "     ['female', 'Italy', 'football'],\n",
    "     ['female', 'Italy', 'painting'],\n",
    "     ['male', 'Spain', 'tennis'],\n",
    "     ['female', 'France', 'football'],\n",
    "     ['male', 'Italy', 'painting']]\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzqxRe1oE-S4"
   },
   "outputs": [],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1ZE2KE4FF7u"
   },
   "outputs": [],
   "source": [
    "enc.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eauJijGZD4wQ"
   },
   "outputs": [],
   "source": [
    "enc.transform([['female', 'USA', 'football']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxfweGfSKhh_"
   },
   "outputs": [],
   "source": [
    "enc.inverse_transform([[1., 0., 0., 0., 1., 1., 0., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4In4aHASByy"
   },
   "source": [
    "### OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJbsknFWJPSk"
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV6nyPrmJPY-"
   },
   "outputs": [],
   "source": [
    "X = [['male', 'Spain', 'tennis'],\n",
    "     ['female', 'Italy', 'football'],\n",
    "     ['female', 'Italy', 'painting'],\n",
    "     ['male', 'Spain', 'tennis'],\n",
    "     ['female', 'France', 'football'],\n",
    "     ['male', 'Italy', 'painting']]\n",
    "\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5uFg2uUJnlf"
   },
   "outputs": [],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWs0ctnKJPfS"
   },
   "outputs": [],
   "source": [
    "enc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIGEMJvqJ4bt"
   },
   "outputs": [],
   "source": [
    "enc.transform([['female','Spain', 'football']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UyU3E91NkH1"
   },
   "source": [
    "## Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jo-Rb1BNkH1"
   },
   "source": [
    "Discretization (otherwise known as quantization or binning) provides a way to partition **continuous features into discrete values**. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3FanZC4NkH1"
   },
   "outputs": [],
   "source": [
    "X = np.array([[ -3., 5., 15 ],\n",
    "              [  0., 6., 14 ],\n",
    "              [  6., 3., 11 ]])\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 3], encode='ordinal').fit(X)\n",
    "est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekpwBnFYNkH3"
   },
   "outputs": [],
   "source": [
    "est.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YOfvowz1x6E"
   },
   "source": [
    "There are different strategies implemented in KBinsDiscretizer:\n",
    "\n",
    "- ‘uniform’: The discretization is uniform in each feature, which means that the bin widths are constant in each dimension.\n",
    "\n",
    "- ‘quantile’: The discretization is done on the quantiled values, which means that each bin has approximately the same number of samples.\n",
    "\n",
    "- ‘kmeans’: The discretization is based on the centroids of a KMeans clustering procedure.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization_strategies.html?highlight=kbinsdiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ1Oy1rgNkH6"
   },
   "source": [
    "Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zd_zOlecNkH7"
   },
   "outputs": [],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer = preprocessing.Binarizer().fit(X)  # threshold default value = 0\n",
    "binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRbcOLTLNkH-"
   },
   "outputs": [],
   "source": [
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InpepFE2NkIB"
   },
   "outputs": [],
   "source": [
    "binarizer = preprocessing.Binarizer(threshold=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu8vbEzfNkID"
   },
   "outputs": [],
   "source": [
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ep7XWWUYNkIF"
   },
   "source": [
    "## Custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fdlq1gnNkIF"
   },
   "source": [
    "Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkTR2GPGNkIF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz-dmUSZSaG8"
   },
   "source": [
    "The *validate* parameter indicates that the input X array should be checked before calling func. The possibilities are:\n",
    "\n",
    "- If False, there is no input validation.\n",
    "\n",
    "- If True, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If the conversion is not possible an exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGFhTXpx2enr"
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlokF9eX2gD1"
   },
   "outputs": [],
   "source": [
    "type(transformer.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ9HW5OElk80"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "di52gFG-E-Yr"
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNUjRc8USQdm"
   },
   "outputs": [],
   "source": [
    "type(transformer.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZOFk14WSUmC"
   },
   "outputs": [],
   "source": [
    "transformer = FunctionTransformer(np.log1p, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AoY6Ic0SYm3"
   },
   "outputs": [],
   "source": [
    "type(transformer.transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWhGL-Vy2VGo"
   },
   "source": [
    "**Pay Attention!\n",
    "The result of a transformer is typically a np.array!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4h19P1Z20lu"
   },
   "outputs": [],
   "source": [
    "preprocessing.StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9-qct0m30rZ"
   },
   "outputs": [],
   "source": [
    "preprocessing.StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StdVCn0pxQ5x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6UyU3E91NkH1",
    "ep7XWWUYNkIF"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "bdta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
