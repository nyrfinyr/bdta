{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KCRwbZeYEDeh"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"W2-_s8blEDej"},"source":["The image below provides a summary of the metrics available in the Scikit-Learn library grouped by task (e.g., classification, regression and clustering).\n","\n","<img src=\"https://drive.google.com/uc?id=184NgFeVKI2uLD5eyKessu4TmopKk-q9w\">"]},{"cell_type":"markdown","metadata":{"id":"teoqikLXEDej"},"source":["# Classification metrics\n","Before diving in the main evaluation metrics lets understand the terms that form the basis for these.\n","\n","- True Positive (TP - actual = 1, predicted = 1): Label which was predicted Positive and is actually Positive.\n","- True Negative (TN - actual = 0, predicted = 0): Label which was predicted Negative and is actually Negative.\n","- False Positive (FP - actual = 0, predicted = 1): Label which was predicted as Positive, but is actually Negative.\n","- False Negative (FN - actual = 1, predicted = 0): Label which was predicted as Negative, but is actually Positive.\n","    \n","<img src=\"https://drive.google.com/uc?id=181x7zkHwtDM3SU2hlz1b0o4ReiTO6Uga\">\n","<img src=\"https://drive.google.com/uc?id=1Kgvbgm_qdAen2Icl2CldERaqCEiYR9nW\">"]},{"cell_type":"markdown","metadata":{"id":"rLlp1qHPEDek"},"source":["## Accuracy score\n","The most common metric for classification is accuracy, which is the **fraction of samples predicted correctly**. In Sklearn the accuracy is computed by the function accuracy_score, which can produce either the fraction (default) or the count (normalize=False) of correct predictions.\n","\n","In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n","\n","If $\\hat{y_i}$ is the predicted value of the i-th sample and $y_i$ is the corresponding true value, then the fraction of correct predictions over $n_{samples}$ is defined as\n","\n","$$accuracy(y_i, \\hat{y_i}) = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}-1} 1(y_i = \\hat{y_i})$$\n","\n","where $1(x)$ is the indicator function."]},{"cell_type":"code","metadata":{"id":"hMnLyCusEDel"},"source":["import numpy as np\n","from sklearn.metrics import accuracy_score\n","y_pred = [0, 2, 1, 3]\n","y_true = [0, 1, 2, 3]\n","print(accuracy_score(y_true, y_pred))\n","#print(accuracy_score(y_true, y_pred, normalize=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HHGWv7aFEDer"},"source":["### Does accuracy matter?\n","Is accuracy an always effective evaluation measure? No! Accuracy is not always the best metric to use to assess classification models. For example, let’s say that we are trying to predict something that only happens 1 out of 100 times. We could build a model that gets 99% accuracy by saying the event never happened. However, we catch 0% of the events we care about. This aspect is particularly critical when your target class is not balanced (data is skewed)."]},{"cell_type":"markdown","metadata":{"id":"_ki6qJszEDes"},"source":["## Precision\n","It is the ‘Exactness’, ability of the model to return only relevant instances. If your use case/problem statement involves minimizing the False Positives then Precision is something you need.\n","\n","$$ Precision = \\frac{TP}{TP+FP} $$"]},{"cell_type":"markdown","metadata":{"id":"mD2ae90EEDet"},"source":["## Recall\n","It is the ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n","\n","$$ Recall = \\frac{TP}{TP+FN} $$\n","\n","One method to boost the recall is to increase the number of samples that you define as predicted positive by lowering the threshold for predicted positive. Unfortunately, this will also increase the number of false positives."]},{"cell_type":"markdown","metadata":{"id":"OIpvQOSnEDe7"},"source":["## F1 score\n","What would you do if one model was better at recall and the other was better at precision? One method that some data scientists use is called the F1 score.\n","The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:\n","\n","$$ F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = \\frac{2*precision*recall}{precision + recall} $$\n","\n","F1 Score reaches its best value at 1 (perfect precision and recall) and worst at 0."]},{"cell_type":"code","metadata":{"id":"MnelDRUthcIs"},"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","y_pred = [0, 1, 1, 0]\n","y_true = [0, 1, 0, 1]\n","print(precision_score(y_true, y_pred))\n","print(recall_score(y_true, y_pred))\n","print(f1_score(y_true, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2rmoeGgEDe9"},"source":["## Multiclass classification\n","In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the average argument to the average_precision_score (multilabel only), f1_score, fbeta_score, precision_recall_fscore_support, precision_score and recall_score functions, as described above. Note that if all labels are included, “micro”-averaging in a multiclass setting will produce precision, recall and $F$ that are all identical to accuracy. Also note that “weighted” averaging may produce an F-score that is not between precision and recall.\n","\n","<img src=\"https://drive.google.com/uc?id=17jk9jEcha3DVebkNseD-yYJSWSyiQJ2M\">"]},{"cell_type":"markdown","metadata":{"id":"xbfWHdVHEDfD"},"source":["## Receiver operating characteristic (ROC) and AUC\n","A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\n","\n","ROC curves are very help with understanding the **balance between true positive rate and false positive rates**. Sci-kit learn has built in functions for ROC curves and for analyzing them. The inputs to these functions (roc_curve and roc_auc_score) are the actual labels and the predicted probabilities (not the predicted labels).\n","\n","The roc_curve function returns three lists:\n","- thresholds = all unique prediction probabilities in descending order\n","- TPR = the true positive rate (TP / (TP + FN)) for each threshold (sensitivity)\n","- FPR = the false positive rate (FP / (FP + TN)) for each threshold (1-specificity)"]},{"cell_type":"code","metadata":{"id":"Qh-TvY_QEDfE"},"source":["import numpy as np\n","from sklearn.metrics import roc_curve\n","y = np.array([1, 1, 2, 2])\n","scores = np.array([0.1, 0.4, 0.35, 0.8])\n","fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n","print(fpr)\n","print(tpr)\n","print(thresholds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YIRGIJeEDfJ"},"source":["import matplotlib.pyplot as plt\n","plt.plot(fpr, tpr,'r-',label = 'Test')\n","#plt.plot([0,1],[0,1],'k-',label='Random')\n","#plt.plot([0,0,1,1],[0,1,1,1],'g-',label='Perfect')\n","plt.legend()\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1jzzC_a54MqYxbyz37NpPJT3E_ElUamMX\">"],"metadata":{"id":"yo-YwsMSn05Y"}},{"cell_type":"markdown","metadata":{"id":"T0SdQO2aEDfM"},"source":["There are a couple things that we can observe from this figure:\n","- a model that randomly guesses the label will result in the red line and you want to have a model that has a curve above this red line\n","- an ROC that is farther away from the red line is better, so green/blue look better than red\n","- Although not seen directly, a high threshold results in a point in the bottom left and a low threshold results in a point in the top right. This means as you decrease the threshold you get higher TPR at the cost of a higher FPR\n","\n","To analyze the performance, we will use the area-under-curve metric. The roc_auc_score function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number.\n"]},{"cell_type":"code","metadata":{"id":"BWBUc466EDfN"},"source":["import numpy as np\n","from sklearn.metrics import roc_auc_score\n","y_true = np.array([0, 0, 1, 1])\n","y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n","roc_auc_score(y_true, y_scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M82nHoSzEDfd"},"source":["## Confusion matrix\n","A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n","\n","The confusion matrix shows the ways in which your classification model is confused when it makes predictions.\n","\n","It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n","\n","By definition, entry $i,j$ in a confusion matrix is the number of observations actually in group $i$, but predicted to be in group $j$. Here is an example:"]},{"cell_type":"code","metadata":{"id":"WMBlrKAWEDfe"},"source":["from sklearn.metrics import confusion_matrix\n","y_true = [2, 0, 2, 2, 0, 1]\n","y_pred = [0, 0, 2, 2, 0, 2]\n","confusion_matrix(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDyeLxPFzJE-"},"source":["confusion_matrix(y_true, y_pred)[1,1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3manFckgEDfh"},"source":["### Example\n","Example of confusion matrix usage to evaluate the quality of the output of a classifier on the iris data set. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.\n","\n","The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.\n","\n","Here the results are not as good as they could be as our choice for the regularization parameter C was not the best."]},{"cell_type":"code","metadata":{"id":"4IRgMiqEEDfh"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn import svm, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","\n","# import some data to play with\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","class_names = iris.target_names\n","\n","# Split the data into a training set and a test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","\n","# Run classifier, using a model that is too regularized (C too low) to see\n","# the impact on the results\n","classifier = svm.SVC(kernel='linear', C=0.01)\n","y_pred = classifier.fit(X_train, y_train).predict(X_test)\n","\n","\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    classes = classes[unique_labels(y_true, y_pred)]\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return ax\n","\n","\n","np.set_printoptions(precision=2)\n","\n","# Plot non-normalized confusion matrix\n","plot_confusion_matrix(y_test, y_pred, classes=class_names,\n","                      title='Confusion matrix, without normalization')\n","\n","# Plot normalized confusion matrix\n","plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n","                      title='Normalized confusion matrix')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OA4O3x-WEDfl"},"source":["For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:"]},{"cell_type":"code","metadata":{"id":"WzBXgf8kEDfl"},"source":["y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n","y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n","tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","tn, fp, fn, tp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FfOFmaxEjVpP"},"source":["confusion_matrix(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLy4lG50EDfu"},"source":["## Classification report\n","The classification_report function builds a text report showing the main classification metrics. Here is a small example with custom target_names and inferred labels:"]},{"cell_type":"code","metadata":{"id":"2xmwxfvFEDfu"},"source":["from sklearn.metrics import classification_report\n","y_true = [0, 1, 2, 2, 0]\n","y_pred = [0, 0, 2, 1, 0]\n","target_names = ['class 0', 'class 1', 'class 2']\n","print(classification_report(y_true, y_pred, target_names=target_names))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JexgdYJCEDfx"},"source":["# Regression metrics\n","The sklearn.metrics module implements several loss, score, and utility functions to measure regression performance."]},{"cell_type":"markdown","metadata":{"id":"TngrQBpwEDfy"},"source":["## Explained variance score\n","The explained_variance_score computes the explained variance regression score.\n","\n","If $\\hat{y}$ is the estimated target output, $y$ the corresponding (correct) target output, and $Var$ is Variance, the square of the standard deviation, then the explained variance is estimated as follow:\n","\n","$$ explained\\_variance(\\hat{y}, y) = 1 - \\frac{Var\\{y - \\hat{y}\\}}{Var\\{y\\}} $$\n","\n","The best possible score is 1.0, lower values are worse."]},{"cell_type":"code","metadata":{"id":"Y3ht5E0SEDfy"},"source":["from sklearn.metrics import explained_variance_score\n","y_true = [3, -0.5, 2, 7]\n","# y_pred = [2.5, 0.0, 2, 8]\n","y_pred = [12.5, 1005.0, 12, 18]\n","print(explained_variance_score(y_true, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t4AQ10twEDf2"},"source":["## Max error\n","The max_error function computes the maximum residual error , a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, max_error would be 0 on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.\n","\n","If $\\hat{y_i}$ is the predicted value of the i-th sample, and $y_i$ is the corresponding true value, then the max error is defined as:\n","\n","$$ MaxError(\\hat{y}, y) = max(|y_i-\\hat{y_i}|) $$\n","\n"]},{"cell_type":"code","metadata":{"id":"3FkumiuREDf3"},"source":["from sklearn.metrics import max_error\n","y_true = [3, 2, 7, 1]\n","y_pred = [9, 2, 7, 2]\n","max_error(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJzy4WVJEDf7"},"source":["## Mean Absolute Error\n","The mean_absolute_error function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss or l1-norm loss.\n","\n","If $\\hat{y_i}$ is the predicted value of the i-th sample, and $y_i$ is the corresponding true value, then the mean absolute error (MAE) estimated over $n_{samples}$ is defined as:\n","\n","$$ MAE(y, \\hat{y}) = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}} |y_{i}-\\hat{y_{i}}| $$"]},{"cell_type":"markdown","metadata":{"id":"BNvignH_ikXh"},"source":["**MAE is a linear score which means all the individual differences are weighted equally. In this perspective, it is robust to outliers and does not penalize the errors, and it is not suitable for applications where you want to pay more attention to the outliers.**\n","\n","**Few positive values might bring the error up while few negatives bring the error down finally resulting in a statistic not indicative of model performance. So, we consider only the difference in magnitude of actual and predicted.**"]},{"cell_type":"code","metadata":{"id":"YNY_VL0DEDf8"},"source":["from sklearn.metrics import mean_absolute_error\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 12]\n","mean_absolute_error(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7MboGKCEDf_"},"source":["## Mean Squared Error\n","The mean_squared_error function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.\n","\n","If $\\hat{y_i}$ is the predicted value of the i-th sample, and $y_i$ is the corresponding true value, then the mean squared error (MSE) estimated over $n_{samples}$ is defined as:\n","\n","$$ MSE(y, \\hat{y}) = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}} (y_{i}-\\hat{y_{i}})^2 $$"]},{"cell_type":"code","metadata":{"id":"tPA2hYSs7dbm"},"source":["from sklearn.metrics import mean_squared_error\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 12]\n","mean_squared_error(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leOT9nXAoyDE"},"source":["**As it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is. It is preferred more than other metrics because it is differentiable and hence can be optimized better.**\n","\n","**Another way to interpret MSE is Variance of error values (how widely dispersed errors are)!**"]},{"cell_type":"markdown","metadata":{"id":"ZAxkn-PEoMoK"},"source":["## Root Mean Square Error\n","RMSE is the most widely used metric for regression tasks and is the square root of the averaged squared difference between the target value and the value predicted by the model. It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors. This implies that **RMSE is useful when large errors are undesired**.\n","\n","$$ RMSE(y, \\hat{y}) = \\sqrt{\\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}} (y_{i}-\\hat{y_{i}})^2} $$\n","\n","It represents the sample standard deviation of the differences between predicted values and observed values (called residuals)."]},{"cell_type":"code","metadata":{"id":"yv55sTvJEDgB"},"source":["from sklearn.metrics import mean_squared_error\n","from math import sqrt\n","\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","sqrt(mean_squared_error(y_true, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xznDzx6irBG4"},"source":["## MAE vs RMSE\n","It is easy to understand and interpret MAE because it directly takes the average of offsets whereas RMSE penalizes the higher difference more than MAE.\n","Let’s understand the above statement with the two examples:\n","\n","Case 1: Actual Values = [2,4,6,8] , Predicted Values = [4,6,8,10]\n","\n","Case 2: Actual Values = [2,4,6,8] , Predicted Values = [4,6,8,12]\n","\n","MAE for case 1 = 2.0, RMSE for case 1 = 2.0\n","\n","MAE for case 2 = 2.5, RMSE for case 2 = 2.65\n","\n","From the above example, we can see that RMSE penalizes the last value prediction more heavily than MAE. Generally, RMSE will be higher than or equal to MAE. The only case where it equals MAE is when all the differences are equal or zero (true for case 1 where the difference between actual and predicted is 2 for all observations).\n","\n","The range of RMSE and MAE is from 0 to infinity."]},{"cell_type":"markdown","metadata":{"id":"quroDoznEDgF"},"source":["## Mean Squared Logarithmic Error\n","The mean_squared_log_error function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.\n","\n","If $\\hat{y_i}$ is the predicted value of the i-th sample, and $y_i$ is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over $n_{samples}$ is defined as:\n","\n","$$ MSLE(y, \\hat{y}) = \\frac{1}{n_{samples}} \\sum_{i=0}^{n_{samples}} (log_e(1+y_{i})-log_e(1+\\hat{y_{i}}))^2 $$\n","\n","Where $log_e(x)$ means the natural logarithm of $x$. This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate."]},{"cell_type":"markdown","metadata":{"id":"Ia0l2B0pmteT"},"source":["**When observations are huge in magnitude for both actual and predicted values, error for that pair is going to be large compared to other smaller magnitude observations. For instance, you might come across a real estate dataset where there is a good mix of expensive mansions, average houses and ultra-cheap falling apart houses like these. If a model predicts small condos worth 100,000 as 50,000 then it is off by a lot but if the same model predicts a mansion’s price as 900,000 instead of 850,000 we can consider it close. The same error value of $50k is both massive and also insignificant in the same data set. So, in such cases, to avoid such relatively large differences between actual and predicted value contributing to error, we use MSLE.**"]},{"cell_type":"code","metadata":{"id":"jVl0y74UEDgG"},"source":["from sklearn.metrics import mean_squared_log_error\n","y_true = [3, 5, 2.5, 7]\n","y_pred = [2.5, 5, 4, 14]\n","mean_squared_log_error(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"998peSVQEDgI"},"source":["## Median Absolute Error\n","The median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.\n","\n","If $\\hat{y_i}$ is the predicted value of the i-th sample, and $y_i$ is the corresponding true value, then the median absolute error (MedAE) estimated over $n_{samples}$ is defined as:\n","\n","$$ MedAE(y, \\hat{y}) = median(|y_{1}-\\hat{y_{1}}|, ..., |y_{n}-\\hat{y_{n}}|) $$"]},{"cell_type":"code","metadata":{"id":"2K_IS1sIEDgJ"},"source":["from sklearn.metrics import median_absolute_error\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","median_absolute_error(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mm3WEtGgEDgM"},"source":["## R² score, the coefficient of determination\n","The r2_score function computes the coefficient of determination, usually denoted as R².\n","\n","It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.\n","\n","As such variance is dataset dependent, R² may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R² score of 0.0.\n","\n","If $\\hat{y_i}$ is the predicted value of the i-th sample, and $y_i$ is the corresponding true value for total $n$ samples, the estimated $R^2$ is defined:\n","\n","$$ R^2(y, \\hat{y}) = 1-\\frac{\\sum_{i=0}^{n}(y_i-\\hat{y_i})^2}{Var\\{y\\}} = 1 - \\frac{MSE(model)}{MSE(baseline)}$$"]},{"cell_type":"markdown","metadata":{"id":"AjYqPVvVnwap"},"source":["**The metric helps us to compare our current model with a constant baseline and tells us how much our model is better. The constant baseline is chosen by taking the mean of the data and drawing a line at the mean. R² is a scale-free score that implies it doesn't matter whether the values are too large or too small, the R² will always be less than or equal to 1.**\n","\n","**There is a misconception among people that R² score ranges from 0 to 1 but actually it ranges from -∞ to 1. Due to this misconception, they are sometimes scared why the R² is negative which is not a possibility according to them.**\n","\n","**One of the main reason for R² to be negative is that the chosen model does not follow the trend of the data causing the R² to be negative. This causes the mse of the chosen model(numerator) to be more than the mse for constant baseline(denominator) resulting in negative R².**"]},{"cell_type":"code","metadata":{"id":"lvaVllGTEDgN"},"source":["from sklearn.metrics import r2_score\n","y_true = [3, -0.5, 2, 7]\n","y_pred = [2.5, 0.0, 2, 8]\n","r2_score(y_true, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"tzGldcl5EDgQ"},"source":["# Cross-validation: evaluating estimator performance\n","Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called __overfitting__. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. The best parameters can be determined by grid search techniques.\n","\n","\n","In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split helper function. Let’s load the iris data set to fit a linear support vector machine on it:"]},{"cell_type":"code","metadata":{"id":"MaispwLVEDgS"},"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from sklearn import svm\n","\n","iris = datasets.load_iris()\n","iris.data.shape, iris.target.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6j4oUJmOEDgW"},"source":["We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:"]},{"cell_type":"code","metadata":{"id":"Qwb0uMnIEDgW"},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    iris.data, iris.target, test_size=0.4, random_state=5)\n","\n","print(X_train.shape, y_train.shape)\n","\n","print(X_test.shape, y_test.shape)\n","\n","\n","clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n","print(clf.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fI5SG0pzEDga"},"source":["When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n","\n","However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n","\n","A solution to this problem is a procedure called __cross-validation__ (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n","\n","- A model is trained using k-1 of the folds as training data;\n","- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n","The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as in the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n","\n","<img src=\"https://drive.google.com/uc?id=187PlX9bDDNCQyO0sPTpBupluR2LaJ_tM\">"]},{"cell_type":"markdown","metadata":{"id":"YRlXHgMeEDgb"},"source":["## Computing cross-validated metrics\n","The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n","\n","The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):"]},{"cell_type":"code","metadata":{"id":"wmGTu73wEDgc"},"source":["from sklearn.model_selection import cross_val_score\n","\n","clf = svm.SVC(kernel='linear', C=1)\n","\n","scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n","print(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YeDlbRnm0ttF"},"source":["scores.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rk53MebFEDgg"},"source":["print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUHiK3IdEDgk"},"source":["By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter:"]},{"cell_type":"code","metadata":{"id":"E6YB68r6EDgl"},"source":["from sklearn import metrics\n","scores = cross_val_score(\n","    clf, iris.data, iris.target, cv=5, scoring='recall_macro')\n","print(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-pISI64YEDgt"},"source":["The **cross_validate function** differs from cross_val_score in two ways:\n","\n","- It allows specifying multiple metrics for evaluation.\n","- It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.\n","\n","The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:"]},{"cell_type":"code","metadata":{"id":"PB_E50bpEDgt"},"source":["from sklearn.model_selection import cross_validate\n","from sklearn.metrics import recall_score\n","\n","scoring = ['precision_macro', 'recall_macro','accuracy']\n","\n","clf = svm.SVC(kernel='linear', C=1, random_state=0)\n","\n","scores = cross_validate(clf, iris.data, iris.target, scoring=scoring, cv=5)\n","\n","print(sorted(scores.keys()))\n","print(scores['test_recall_macro'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0qMj7Of1hJK"},"source":["scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RHUXhulSEDgy"},"source":["Here is an example of cross_validate using a single metric:"]},{"cell_type":"code","metadata":{"id":"g4cJIUDgEDgz"},"source":["scores = cross_validate(clf, iris.data, iris.target,\n","                        scoring='precision_macro', cv=5,\n","                        return_estimator=True)\n","print(sorted(scores.keys()))\n","print(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scores"],"metadata":{"id":"2nMAxv-8_ryH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LhAq9IAEDg3"},"source":["## Cross validation iterators\n","The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.\n","Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.\n","\n","The following cross-validators can be used in such cases."]},{"cell_type":"markdown","metadata":{"id":"u11CwgpVEDg4"},"source":["### K-fold\n","KFold divides all the samples in $k$ groups of samples, called folds (if $k=n$, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using $k-1$ folds, and the fold left out is used for test.\n","\n","Example of 2-fold cross-validation on a dataset with 4 samples:"]},{"cell_type":"code","metadata":{"id":"RDx0T8IBEDg5"},"source":["import numpy as np\n","from sklearn.model_selection import KFold\n","\n","X = np.array([\"a\", \"b\", \"c\", \"d\"])\n","kf = KFold(n_splits=3)\n","for train, test in kf.split(X):\n","    print(\"%s %s\" % (train, test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MlMtTphnytwo"},"source":["train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxKQ6oy4TIBK"},"source":["X[train]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"utcnaSiuEDg9"},"source":["### Repeated K-fold\n","RepeatedKFold repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition.\n","\n","Example of 2-fold K-Fold repeated 2 times:"]},{"cell_type":"code","metadata":{"id":"E6EgLsrYEDg-"},"source":["import numpy as np\n","from sklearn.model_selection import RepeatedKFold\n","X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n","random_state = 12883823\n","rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n","for train, test in rkf.split(X):\n","    print(\"%s %s\" % (train, test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRedUTHlEDhC"},"source":["### Leave One Out (LOO)\n","LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for  samples, we have  different training sets and  different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:"]},{"cell_type":"code","metadata":{"id":"t4gefle7EDhC"},"source":["from sklearn.model_selection import LeaveOneOut\n","\n","#X = np.array([1, 2, 3, 4])\n","X = np.array([\"a\", \"b\", \"c\", \"d\"])\n","loo = LeaveOneOut()\n","for train, test in loo.split(X):\n","    print(\"%s %s\" % (train, test))\n","    #print(X[train])\n","    #print(X[test])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DoNJaHTZ2SqO"},"source":["X[train]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDjrpM0qEDhI"},"source":["Potential users of LOO for model selection should weigh a few known caveats. When compared with k-fold cross validation, one builds $n$ models from $n$ samples instead of $k$ models, where $n>k$. Moreover, each is trained on $n-1$ samples rather than $(k-1)n/k$. In both ways, assuming $k$ is not too large and $k<n$, LOO is more computationally expensive than k-fold cross validation.\n","\n","In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since $n-1$ of the $n$ samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.\n","\n","However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.\n","\n","As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO."]},{"cell_type":"markdown","metadata":{"id":"zMFNRdmqEDhI"},"source":["### Leave P Out (LPO)\n","LeavePOut is very similar to LeaveOneOut as it creates all the possible training/test sets by removing $p$ samples from the complete set. For $n$ samples, this produces $\\binom{n}{p}$ train-test pairs. Unlike LeaveOneOut and KFold, the test sets will overlap for $p>1$. Example of Leave-2-Out on a dataset with 4 samples:"]},{"cell_type":"code","metadata":{"id":"4AF29yn5EDhJ"},"source":["from sklearn.model_selection import LeavePOut\n","\n","X = np.ones(4)\n","lpo = LeavePOut(p=2)\n","for train, test in lpo.split(X):\n","    print(\"%s %s\" % (train, test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOQd5jH33Flv"},"source":["X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QYJGxTGEDhL"},"source":["### Random permutations cross-validation a.k.a. Shuffle & Split\n","The ShuffleSplit iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.\n","\n","It is possible to control the randomness for reproducibility of the results by explicitly seeding the random_state pseudo random number generator.\n","\n","Here is a usage example:"]},{"cell_type":"code","metadata":{"id":"iW9Gn18gEDhL"},"source":["from sklearn.model_selection import ShuffleSplit\n","X = np.arange(10)\n","ss = ShuffleSplit(n_splits=5, test_size=0.25,\n","    random_state=0)\n","for train_index, test_index in ss.split(X):\n","    print(\"%s %s\" % (train_index, test_index))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vzNrBT5KEDhO"},"source":["ShuffleSplit is thus a good alternative to KFold cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split."]},{"cell_type":"markdown","metadata":{"id":"IPCLC_itEDhP"},"source":["###  StratifiedKFold\n","Some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in StratifiedKFold and StratifiedShuffleSplit to ensure that relative class frequencies is approximately preserved in each train and validation fold.\n","\n","StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n","\n","Example of stratified 3-fold cross-validation on a dataset with 10 samples from two slightly unbalanced classes:"]},{"cell_type":"code","metadata":{"id":"arN3MRr4EDhP"},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","X = np.ones(10)\n","y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n","skf = StratifiedKFold(n_splits=3)\n","for train, test in skf.split(X, y):\n","    print(\"%s %s\" % (train, test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L70JIhJAEDhS"},"source":["RepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times with different randomization in each repetition."]},{"cell_type":"markdown","metadata":{"id":"KUQJpDouVtcp"},"source":["# Dummy estimators\n","When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. DummyClassifier implements several such simple strategies for classification:\n","\n","- stratified generates random predictions by respecting the training set class distribution.\n","- most_frequent always predicts the most frequent label in the training set.\n","- uniform generates predictions uniformly at random.\n","- constant always predicts a constant label that is provided by the user.\n","A major motivation of this method is F1-scoring, when the positive class is in the minority.\n","\n","Note that with all these strategies, the predict method completely ignores the input data!\n","\n","To illustrate DummyClassifier, first let’s create an imbalanced dataset:"]},{"cell_type":"code","metadata":{"id":"aZqBH7i5WDBO"},"source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","y[y != 1] = -1\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train"],"metadata":{"id":"Cq6ryKOsbedp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test"],"metadata":{"id":"o8B2CCfYbiXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Hr8o64zWH0x"},"source":["from sklearn.dummy import DummyClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\n","print(clf.score(X_test, y_test))\n","\n","clf = DummyClassifier(strategy='uniform', random_state=0)\n","clf.fit(X_train, y_train)\n","result= clf.predict(X_test)\n","print(accuracy_score(result, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vyKe-skSW12Y"},"source":["We see that SVC doesn’t do much better than a dummy classifier. Now, let’s change the kernel:"]},{"cell_type":"code","metadata":{"id":"ExuLwQP0WH74"},"source":["clf = SVC(gamma='scale', kernel='rbf', C=1).fit(X_train, y_train)\n","clf.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXGKJi9dWsZx"},"source":["We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly.\n","\n","More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc…\n","\n","DummyRegressor also implements four simple rules of thumb for regression:\n","\n","    - mean always predicts the mean of the training targets.\n","    - median always predicts the median of the training targets.\n","    - quantile always predicts a user provided quantile of the training targets.\n","    - constant always predicts a constant value that is provided by the user.\n","In all these strategies, the predict method completely ignores the input data."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"TlY2Y91QEDhS"},"source":["# Tuning the hyper-parameters of an estimator\n","Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes.\n","It is possible and recommended to search the hyper-parameter space for the best cross validation score.\n","\n","Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use __estimator.get_params()__.\n","A search consists of:\n","\n","- an estimator (regressor or classifier such as sklearn.svm.SVC());\n","- a parameter space;\n","- a method for searching or sampling candidates;\n","- a cross-validation scheme; and\n","- a score function.\n","    \n","Some models allow for specialized, efficient parameter search strategies, outlined below. Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, GridSearchCV exhaustively considers all parameter combinations, while RandomizedSearchCV can sample a given number of candidates from a parameter space with a specified distribution. After describing these tools we detail best practice applicable to both approaches."]},{"cell_type":"markdown","metadata":{"id":"yuE324NoEDhT"},"source":["## Exhaustive Grid Search\n","The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid:\n","\n","param_grid = [\n","  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n","  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n"," ]\n","\n","specifies that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second one with an RBF kernel, and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001].\n","\n","The GridSearchCV instance implements the usual estimator API: when “fitting” it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained."]},{"cell_type":"code","metadata":{"id":"n2hsArOnXGBS"},"source":["from sklearn import svm, datasets\n","from sklearn.model_selection import GridSearchCV\n","\n","iris = datasets.load_iris()\n","\n","parameters = {'kernel':('linear', 'rbf','poly'), 'C':[0.1,1, 10]}\n","\n","svc = svm.SVC(gamma=\"scale\")\n","\n","clf = GridSearchCV(svc, parameters, cv=5)\n","clf.fit(iris.data, iris.target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPgREyU0XKj0"},"source":["clf.get_params()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7hjkX4bXLUl"},"source":["clf.cv_results_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GH-9nkyeXNn8"},"source":["clf.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_mc3FpBXS1W"},"source":["clf.best_score_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nbMAoXL8z1n"},"source":["parameters = {'C':[0.1,0.2, 0.09]}\n","svc = svm.SVC(gamma=\"scale\", kernel= \"poly\")\n","\n","clf = GridSearchCV(svc, parameters, cv=5)\n","clf.fit(iris.data, iris.target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5-QP-li9DUp"},"source":["clf.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yfVPh6d9Xhb"},"source":["clf.best_score_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKlXdJKeEDhU"},"source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report\n","from sklearn.svm import SVC\n","\n","print(__doc__)\n","\n","# Loading the Digits dataset\n","digits = datasets.load_digits()\n","\n","# To apply a classifier on this data, we need to flatten the image, to\n","# turn the data in a (samples, feature) matrix:\n","n_samples = len(digits.images)\n","X = digits.images.reshape((n_samples, -1))\n","y = digits.target\n","\n","# Split the dataset in two equal parts\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.5, random_state=0)\n","\n","# Set the parameters by cross-validation\n","tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n","                     'C': [1, 10, 100, 1000]},\n","                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n","\n","scores = ['precision', 'recall']\n","\n","for score in scores:\n","    print(\"# Tuning hyper-parameters for %s\" % score)\n","    print()\n","\n","    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n","                       scoring='%s_macro' % score)\n","    clf.fit(X_train, y_train)\n","\n","    print(\"Best parameters set found on development set:\")\n","    print()\n","    print(clf.best_params_)\n","    print()\n","    print(\"Grid scores on development set:\")\n","    print()\n","    means = clf.cv_results_['mean_test_score']\n","    stds = clf.cv_results_['std_test_score']\n","    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","        print(\"%0.3f (+/-%0.03f) for %r\"\n","              % (mean, std * 2, params))\n","    print()\n","\n","    print(\"Detailed classification report:\")\n","    print()\n","    print(\"The model is trained on the full development set.\")\n","    print(\"The scores are computed on the full evaluation set.\")\n","    print()\n","    y_true, y_pred = y_test, clf.predict(X_test)\n","    print(classification_report(y_true, y_pred))\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vPm3XX22EDhX"},"source":["## Randomized Parameter Optimization\n","While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n","\n","A budget can be chosen independent of the number of parameters and possible values. Adding parameters that do not influence the performance does not decrease efficiency. Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:\n","\n","{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n","\n","This example uses the scipy.stats module, which contains many useful distributions for sampling parameters, such as expon, gamma, uniform or randint. In principle, any function can be passed that provides a rvs (random variate sample) method to sample a value. A call to the rvs function should provide independent random samples from possible parameter values on consecutive calls."]},{"cell_type":"code","metadata":{"id":"5wcDVVp2Xeeh"},"source":["from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import uniform\n","\n","iris = load_iris()\n","\n","logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=0)\n","\n","distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n","\n","clf = RandomizedSearchCV(logistic, distributions, random_state=0, n_iter=20)\n","search = clf.fit(iris.data, iris.target)\n","search.best_params_"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["search.best_estimator_"],"metadata":{"id":"ZMDo3V8CpYio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["search.best_params_"],"metadata":{"id":"46w6-Up1qMaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["search.cv_results_"],"metadata":{"id":"1twDHXSdnIBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uniform(loc=0, scale=4)"],"metadata":{"id":"Yc-dRLEKoe2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6_KaKxekoz3o"},"execution_count":null,"outputs":[]}]}